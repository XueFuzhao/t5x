# Register necessary SeqIO Tasks/Mixtures.
from __gin__ import dynamic_registration
import t5.data.mixtures
import __main__ as train_script
import seqio
from t5x import optimizers
from t5x import utils
import optax


include 't5x/examples/t5/t5_1_1/base.gin'
include 't5x/configs/runs/pretrain.gin'

MIXTURE_OR_TASK_NAME = "c4_v220_span_corruption"
USE_CACHED_TASKS = False
TASK_FEATURE_LENGTHS = {"inputs": 512, "targets": 114}
TRAIN_STEPS = 524288
DROPOUT_RATE = 0.0
BATCH_SIZE = 128

train/utils.DatasetConfig:
  split = 'train[:310635]'

train_script.train:
  eval_period = 2000
  
# In this case, we choose to switch to the AdamW optimizer with gradient clip.
OPTIMIZER = @optimizers.chain()

optimizers.chain:
  transformations = [@optax.clip(), @optax.lamb()]

optax.clip:
  max_delta = 1.0

# optax.adamw:
  # Unlike Adafactor, most optimizers require to specify
  # `learning_rate`. `learning_rate` accepts a float number (e.g., 1e-4) or
  # a schedule function, which should take an argument `step` and output
  # a learning rate for that step.
  # As for choices of schedule functions, we can either use T5x
  # learning rate scheduler, i.e., utils.create_learning_rate_scheduler, or
  # optax's native schedule functions, e.g., warmup_cosine_decay_schedule.
#   learning_rate = @optax.warmup_cosine_decay_schedule()

# optax.warmup_cosine_decay_schedule:
#   init_value = 0.0
#   peak_value = 1e-4
#   warmup_steps = 1000
#   decay_steps = %TRAIN_STEPS
#   end_value = 0.0


# Below is an example of using the T5X's schedule functions.
# Feel free to uncomment to try.
optax.lamb:
  learning_rate = @utils.create_learning_rate_scheduler()

utils.create_learning_rate_scheduler:
  factors = 'constant * rsqrt_decay'
  base_learning_rate = 1.0
  warmup_steps = 10000  # 10k to keep consistent with T5/MTF defaults.
